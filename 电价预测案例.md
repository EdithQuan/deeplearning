先注明要用到的库

```Python
# importing IPython relevant libraries
import warnings
warnings.filterwarnings('ignore')

# import required modules
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import shapiro

# data wrangling and other modules
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.model_selection import train_test_split, learning_curve, GridSearchCV, TimeSeriesSplit
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.decomposition import PCA

# machine learning modules
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from IPython.display import Image
from IPython.core.display import HTML
```

将数据库文件拖到项目里

![image.png](https://flowus.cn/preview/15bb8142-5b52-45ec-816c-58116c64d72e)

定义一下参数，读取文件夹的数据

训练一部分数据

```Python
# Set the seed as a constant to be able to reproduce the experiments properly.
seed = 120

# importing the data set
df = pd.read_csv('power_market.csv', parse_dates=["date"])

# first glimpse on the data set
df.head(10)

df["price"].mean()

# to be used afterwards
date_array = df['date']

df.info()

# get columns with null values and their null count
for column in df:
    if df[column].isnull().any():
       print('{0} has {1} null values'.format(column, df[column].isnull().sum()))

#  print summary info regarding the price column
print(df['price'].describe())

# plot the price distribution
plt.figure(figsize=(5, 5))
sns.distplot(df['price'], color='g', bins=100, hist_kws={'alpha': 0.4});

fig, ax = plt.subplots(figsize=(15, 12))
sns.heatmap(df.corr(), annot=True);

df.tail()

def create_thermal_gap(df):
    df['thermal_gap'] = df['fc_demand'] - (df['fc_nuclear'] + df['fc_solar_pv'] + df['fc_solar_th'] + df['fc_wind'])
    return df

def create_net_exc_power(df):
    # impute and create a difference column with the median
    imp = SimpleImputer(missing_values=np.nan, strategy='median')
    df["export_FR"]  = imp.fit_transform(df[["export_FR"]])
    df["import_FR"]  = imp.fit_transform(df[["import_FR"]])
    df["export_FR"] = df["export_FR"].astype(int)
    df["import_FR"] = df["import_FR"].astype(int)
    df['net_exc_power'] = df['export_FR'] - df['import_FR']
    df['net_exc_power'] = df['net_exc_power'].astype(int) # convert the column to int
    df = df.drop(columns=["import_FR", "export_FR"]) # drop import_FR and export_FR columns after manipulation
    return df

def remove_irrelevant_columns(df):
    df = df.drop(columns=["fc_demand", "date", "fc_solar_pv"]) # drop fc_demand and date
    return df

# pipelining function to be used for each input data
def apply_feature_engineering(df):
    df = create_thermal_gap(df)
    df = create_net_exc_power(df)
    df = remove_irrelevant_columns(df)
    return df

# apply all feature engineering functions
df = apply_feature_engineering(df)

# correlation heatmap after feature engineering
fig, ax = plt.subplots(figsize=(20, 15))
sns.heatmap(df.corr(), annot=True, linewidths=.5, ax=ax)

train_test_split_ratio = 0.17


# separate feature and target columns
X = df.drop(columns=["price"])
y = df['price']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=train_test_split_ratio, random_state=seed)

X_train
```

建立通道和模型

运用多个算法

```Python
clf = Pipeline([
    #("scaler", StandardScaler()), # scaled values before PCA
    #("pca", PCA()), PCA gives us 6 with linear regression
    ("regression", LinearRegression())
])

param_grid= [
    {
        # Linear Regression
        # "pca__n_components": list(range(2, 7)), # Uncomment if PCA is needed
        "regression": [LinearRegression()]
        # Remove the "regression__normalize" part, as LinearRegression does not have this parameter
    },
    {
        # Decision Tree Regression
        "regression": [DecisionTreeRegressor()],
        "regression__max_depth": [15],  # 5, 10, 20 tried
        "regression__random_state": [seed],
    },
    {
        # Random Forest Regression
        "regression": [RandomForestRegressor()],
        "regression__random_state": [seed],
        "regression__max_depth": [10, 15],  # 5, 20 tried
        "regression__n_estimators": [300, 400],  # 30, 50, 100, 200 tried
        "regression__max_features": [0.5]  # 0.6, 0.7, 0.8 tried
    },
    {
        # XGBoost Regression
        "regression": [XGBRegressor()],
        "regression__random_state": [seed],
        "regression__max_depth": [15],  # 5, 10, 20 tried
        "regression__n_estimators": [100],  # 200 tried
        "regression__learning_rate": [0.1],  # 0.01 tried
        "regression__early_stopping_rounds": [15]  # 20 tried
    }
]

clf = GridSearchCV(clf, param_grid, n_jobs=-1, cv=5, scoring="neg_root_mean_squared_error")
clf.fit(X_train, y_train)
```

打印出结果

```Python
# List all the scores
test_scores = clf.cv_results_['mean_test_score'] * (-1)
print("All Test Scores:", test_scores)

# Store the best model in a variable
best_model = clf.best_estimator_
print("Best Model:", best_model)


scoring = pd.read_csv("scoring.csv", parse_dates=["date"])
len(X.columns)
predictions = best_model.predict(X_test)
df["price"].mean()
predictions.mean()
np.sqrt(mean_squared_error(y_test, predictions))



comparison_df = pd.DataFrame()
comparison_df['price'] = y_test
comparison_df['predictions'] = predictions
comparison_df.head(20)
```

plot图形

```Python
actual_scatter = comparison_df["price"]
prediction_scatter = comparison_df["predictions"]
instance_range = comparison_df.index
fig=plt.figure()

# change the size of plots
fig.set_size_inches(25, 10, forward=True)
ax=fig.add_axes([0,0,1,1])
ax.scatter(actual_scatter, instance_range, color='r', label="Actual")
ax.scatter(prediction_scatter, instance_range, color='b', label="Predictions")
ax.set_xlabel('People Count')
ax.set_ylabel('Instant')
ax.set_title('Prediction vs Actual Comparison')

plt.legend()
plt.show()
```

最后即可获得图形

![image.png](https://flowus.cn/preview/3683256c-9edf-4bdc-a699-994e326f79f6)

![image.png](https://flowus.cn/preview/19dcb3a7-0b92-4b6b-ae90-dc8d115bbf18)















最后附上完整代码

```Python
# importing IPython relevant libraries
import warnings
warnings.filterwarnings('ignore')

# import required modules
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import shapiro

# data wrangling and other modules
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.model_selection import train_test_split, learning_curve, GridSearchCV, TimeSeriesSplit
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.decomposition import PCA

# machine learning modules
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from IPython.display import Image
from IPython.core.display import HTML


# Set the seed as a constant to be able to reproduce the experiments properly.
seed = 120

# importing the data set
df = pd.read_csv('power_market.csv', parse_dates=["date"])

# first glimpse on the data set
df.head(10)

df["price"].mean()

# to be used afterwards
date_array = df['date']

df.info()

# get columns with null values and their null count
for column in df:
    if df[column].isnull().any():
       print('{0} has {1} null values'.format(column, df[column].isnull().sum()))

#  print summary info regarding the price column
print(df['price'].describe())

# plot the price distribution
plt.figure(figsize=(5, 5))
sns.distplot(df['price'], color='g', bins=100, hist_kws={'alpha': 0.4});

fig, ax = plt.subplots(figsize=(15, 12))
sns.heatmap(df.corr(), annot=True);

df.tail()

def create_thermal_gap(df):
    df['thermal_gap'] = df['fc_demand'] - (df['fc_nuclear'] + df['fc_solar_pv'] + df['fc_solar_th'] + df['fc_wind'])
    return df

def create_net_exc_power(df):
    # impute and create a difference column with the median
    imp = SimpleImputer(missing_values=np.nan, strategy='median')
    df["export_FR"]  = imp.fit_transform(df[["export_FR"]])
    df["import_FR"]  = imp.fit_transform(df[["import_FR"]])
    df["export_FR"] = df["export_FR"].astype(int)
    df["import_FR"] = df["import_FR"].astype(int)
    df['net_exc_power'] = df['export_FR'] - df['import_FR']
    df['net_exc_power'] = df['net_exc_power'].astype(int) # convert the column to int
    df = df.drop(columns=["import_FR", "export_FR"]) # drop import_FR and export_FR columns after manipulation
    return df

def remove_irrelevant_columns(df):
    df = df.drop(columns=["fc_demand", "date", "fc_solar_pv"]) # drop fc_demand and date
    return df

# pipelining function to be used for each input data
def apply_feature_engineering(df):
    df = create_thermal_gap(df)
    df = create_net_exc_power(df)
    df = remove_irrelevant_columns(df)
    return df

# apply all feature engineering functions
df = apply_feature_engineering(df)

# correlation heatmap after feature engineering
fig, ax = plt.subplots(figsize=(20, 15))
sns.heatmap(df.corr(), annot=True, linewidths=.5, ax=ax)

train_test_split_ratio = 0.17


# separate feature and target columns
X = df.drop(columns=["price"])
y = df['price']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=train_test_split_ratio, random_state=seed)

X_train


# Pipeline creation and model training


clf = Pipeline([
    #("scaler", StandardScaler()), # scaled values before PCA
    #("pca", PCA()), PCA gives us 6 with linear regression
    ("regression", LinearRegression())
])

param_grid= [
    {
        # Linear Regression
        # "pca__n_components": list(range(2, 7)), # Uncomment if PCA is needed
        "regression": [LinearRegression()]
        # Remove the "regression__normalize" part, as LinearRegression does not have this parameter
    },
    {
        # Decision Tree Regression
        "regression": [DecisionTreeRegressor()],
        "regression__max_depth": [15],  # 5, 10, 20 tried
        "regression__random_state": [seed],
    },
    {
        # Random Forest Regression
        "regression": [RandomForestRegressor()],
        "regression__random_state": [seed],
        "regression__max_depth": [10, 15],  # 5, 20 tried
        "regression__n_estimators": [300, 400],  # 30, 50, 100, 200 tried
        "regression__max_features": [0.5]  # 0.6, 0.7, 0.8 tried
    },
    {
        # XGBoost Regression
        "regression": [XGBRegressor()],
        "regression__random_state": [seed],
        "regression__max_depth": [15],  # 5, 10, 20 tried
        "regression__n_estimators": [100],  # 200 tried
        "regression__learning_rate": [0.1],  # 0.01 tried
        "regression__early_stopping_rounds": [15]  # 20 tried
    }
]

clf = GridSearchCV(clf, param_grid, n_jobs=-1, cv=5, scoring="neg_root_mean_squared_error")
clf.fit(X_train, y_train)







# List all the scores
test_scores = clf.cv_results_['mean_test_score'] * (-1)
print("All Test Scores:", test_scores)

# Store the best model in a variable
best_model = clf.best_estimator_
print("Best Model:", best_model)


scoring = pd.read_csv("scoring.csv", parse_dates=["date"])
len(X.columns)
predictions = best_model.predict(X_test)
df["price"].mean()
predictions.mean()
np.sqrt(mean_squared_error(y_test, predictions))



comparison_df = pd.DataFrame()
comparison_df['price'] = y_test
comparison_df['predictions'] = predictions
comparison_df.head(20)

actual_scatter = comparison_df["price"]
prediction_scatter = comparison_df["predictions"]
instance_range = comparison_df.index
fig=plt.figure()

# change the size of plots
fig.set_size_inches(25, 10, forward=True)
ax=fig.add_axes([0,0,1,1])
ax.scatter(actual_scatter, instance_range, color='r', label="Actual")
ax.scatter(prediction_scatter, instance_range, color='b', label="Predictions")
ax.set_xlabel('People Count')
ax.set_ylabel('Instant')
ax.set_title('Prediction vs Actual Comparison')

plt.legend()
plt.show()
```



